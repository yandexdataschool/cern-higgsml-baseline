{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiggsML baseline solution\n",
    "\n",
    "This code walks you through the baseline solution for higgs competition\n",
    "1. Downloading the data\n",
    "2. Preprocessing\n",
    "3. Training model\n",
    "4. Saving predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data\n",
    "Just a simple bash script to load kaggle higgs dataset.\n",
    "\n",
    "First run may take a few minutes to actually download the data from `opendata.cern.ch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [[ ! -f 'atlas-higgs-challenge-2014-v2.csv' ]]; then \\\n",
    "    wget -c http://opendata.cern.ch/record/328/files/atlas-higgs-challenge-2014-v2.csv.gz ; \\\n",
    "    gunzip 'atlas-higgs-challenge-2014-v2.csv.gz' ; \\\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read as pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('atlas-higgs-challenge-2014-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100005</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "      <td>0.025434</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>0.083414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100006</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100007</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.018636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100008</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.614803</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.296003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100009</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>28.859</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>167.735</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.001502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "5   100005        89.744                       13.550        59.149   116.344   \n",
       "6   100006       148.754                       28.862       107.782   106.130   \n",
       "7   100007       154.916                       10.418        94.714    29.169   \n",
       "8   100008       105.594                       50.559       100.989     4.288   \n",
       "9   100009       128.053                       88.941        69.272   193.392   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                 0.910           124.711                2.666   \n",
       "1              -999.000          -999.000             -999.000   \n",
       "2              -999.000          -999.000             -999.000   \n",
       "3              -999.000          -999.000             -999.000   \n",
       "4              -999.000          -999.000             -999.000   \n",
       "5                 2.636           284.584               -0.540   \n",
       "6                 0.733           158.359                0.113   \n",
       "7              -999.000          -999.000             -999.000   \n",
       "8              -999.000          -999.000             -999.000   \n",
       "9              -999.000          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "5               1.362      61.619      ...                    -2.412   \n",
       "6               2.941       2.545      ...                     0.864   \n",
       "7               2.897       1.526      ...                    -0.715   \n",
       "8               2.904       4.288      ...                  -999.000   \n",
       "9               1.609      28.859      ...                    -2.767   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                   1.240   \n",
       "1                1.158               -999.000                -999.000   \n",
       "2               -2.028               -999.000                -999.000   \n",
       "3             -999.000               -999.000                -999.000   \n",
       "4             -999.000               -999.000                -999.000   \n",
       "5               -0.653                 56.165                   0.224   \n",
       "6                1.450                 56.867                   0.131   \n",
       "7               -1.724               -999.000                -999.000   \n",
       "8             -999.000               -999.000                -999.000   \n",
       "9               -2.514               -999.000                -999.000   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      s          t   \n",
       "1                -999.000          46.226  0.681042      b          t   \n",
       "2                -999.000          44.251  0.715742      b          t   \n",
       "3                -999.000          -0.000  1.660654      b          t   \n",
       "4                -999.000           0.000  1.904263      b          t   \n",
       "5                   3.106         193.660  0.025434      b          t   \n",
       "6                  -2.767         179.877  0.000814      s          t   \n",
       "7                -999.000          30.638  0.005721      s          t   \n",
       "8                -999.000           0.000  1.614803      b          t   \n",
       "9                -999.000         167.735  0.000461      s          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "5      0.083414  \n",
       "6      0.002653  \n",
       "7      0.018636  \n",
       "8      5.296003  \n",
       "9      0.001502  \n",
       "\n",
       "[10 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a glimpse into the dataset\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "- We apply a couple of standard data analysis techniques\n",
    "  * Selecting the feature __(X)__, target __(y)__ and __weight__ columns \n",
    "  * Imputing missing values\n",
    "  * Adding logarithmic features\n",
    "  * Splitting data into training, test and validation sets\n",
    "\n",
    "\n",
    "You may note that some of the features are redundant for the models we are going to use.\n",
    "Nevertheless, they are computed so that you could jump to your own models quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#target column\n",
    "y = df['Label'].values == 's'\n",
    "\n",
    "#features\n",
    "X_raw = df.iloc[:,1:31]\n",
    "\n",
    "#weights\n",
    "weights = df['KaggleWeight'].values\n",
    "\n",
    "#whether sample is in training or test set\n",
    "dataset_mask = df[\"KaggleSet\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill the missing values\n",
    "* Basically replace the missing cells with the most frequent feature values.\n",
    "* This cell may run for a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "#imputer = Imputer(missing_values = -999.0, strategy = 'mean')\n",
    "X_imputed = X_raw.values#imputer.fit_transform(X_raw)\n",
    "X_imputed[X_imputed==-999.0] = 999.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't like the perspective of waiting that long each time, use \n",
    "* `X_imputed.to_csv(\"myfile.csv\")` to save\n",
    "* `X_imputed = pd.DataFrame.from_csv(\"myfile.csv\")` to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logarithmic features\n",
    "* Pick all strictly positive columns and add $ log(1 + v) $ transformations of these features.\n",
    "* This has only a minor effect for tree methods used here, however it will simplify trying out other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X columns with strictly positive features (selected manually)\n",
    "positive_features = (0,1,2,3,4,5,7,8,9,10,12,13,16,19,21,23,26)\n",
    "\n",
    "X_log = np.log(1 + X_imputed[:,positive_features])\n",
    "X = np.hstack((X_imputed, X_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale to equalize feature variances\n",
    "* This too is hardly applicable to the tree ensembles, but other models may benefit from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/test split\n",
    "* Split data into 3 sets: training, validation and the final test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtr,Ytr,Wtr = map(lambda v: v[dataset_mask=='t'], [X,y,weights])\n",
    "Xval,Yval,Wval = map(lambda v: v[dataset_mask=='b'], [X,y,weights])\n",
    "Xtest,Ytest,Wtest = map(lambda v: v[dataset_mask=='v'], [X,y,weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model\n",
    "\n",
    "Here we define and train a custom ensemble of decision trees.\n",
    "* Namely, we use AdaBoost algorithm above Extremely Randomized Trees ensembles.\n",
    "\n",
    "The idea behind [Extremely Randomized Trees](https://pdfs.semanticscholar.org/336a/165c17c9c56160d332b9f4a2b403fccbdbfb.pdf) is to spawn many weak decision trees using random subsamples of data and features, and than average them out in hope to get a quality above any individual tree.\n",
    "\n",
    "[AdaBoost](http://rob.schapire.net/papers/explaining-adaboost.pdf), in it's turn, trains estimators sequentially, each new one laying more emphasis on the data samples that previous estimators had most problems with. This is done through iterative reweighting of data samples based on the ensemble score on these samples.\n",
    "\n",
    "\n",
    "On each iteration of AdaBoost, we train not one, but several extremely randomized trees using AdaBoost-reweighted samples. We use the extremely randomized trees parallelism to spawn a lot of decision trees in less CPU time and AdaBoost steps to 'guide' the new trees to most important samples. \n",
    "\n",
    "\n",
    "We have taken initial inner model parameters from several Kaggle Higgs Boson Competition forum discussions and made a few steps with sheer gut feeling.\n",
    "We have initialized AdaBoost params with numbers whispered by a weasel ghost, and after a few tweaks they turned out to work ~not too badly.\n",
    "\n",
    "The final model has 300*20=6000 low-quality trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "# The inner Extremely Randomized Trees ensemble to be trained at each AdaBoost step\n",
    "rf = ExtraTreesClassifier(\n",
    "            n_estimators = 300,\n",
    "            max_features = 30,\n",
    "            max_depth = 12,\n",
    "            min_samples_leaf = 100,\n",
    "            min_samples_split = 100,\n",
    "            verbose = 1,\n",
    "            n_jobs = -1 #<- this flag allows to use all CPU cores available. Remove if you don't want that.\n",
    ")\n",
    "\n",
    "# The AdaBoost is defined here\n",
    "classifier = AdaBoostClassifier(\n",
    "        n_estimators = 20,\n",
    "        learning_rate = 0.75,\n",
    "        base_estimator = rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.7s remaining: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   30.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.6s remaining: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 287 | elapsed:    0.1s remaining:   23.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.8s remaining: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  35 | elapsed:    0.0s remaining:    1.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.3s remaining: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 131 | elapsed:    0.0s remaining:    5.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.5s remaining: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 122 | elapsed:    0.1s remaining:    8.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.0s remaining: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   28.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.1s remaining: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   26.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.6s remaining: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   26.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.8s remaining: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 194 | elapsed:    0.1s remaining:   11.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.8s remaining: 23.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 232 | elapsed:    0.1s remaining:   17.8s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.1s remaining: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   16.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.2s remaining: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  30 | elapsed:    0.1s remaining:    1.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.8s remaining: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 268 | elapsed:    0.1s remaining:   20.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.8s remaining: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 112 | elapsed:    0.1s remaining:   10.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.4s remaining: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   39.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.8s remaining: 19.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   40.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.1s remaining: 15.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   57.7s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 118 | elapsed:    0.1s remaining:    6.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.2s remaining: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   59.8s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   27.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.9s remaining: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   58.6s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   21.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.2s remaining: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   57.9s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   27.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fit\n",
      "CPU times: user 5h 56min 48s, sys: 37.9 s, total: 5h 57min 26s\n",
      "Wall time: 23min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"model fit\"\n",
    "classifier.fit(Xtr, Ytr, sample_weight = Wtr)\n",
    "classifier.verbose=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate AMS threshold\n",
    "* Approximate Median significance is a threshold-dependent metric\n",
    "* In other words, it requires you to assign \"signal\" vs \"background\" class, not just probabilities.\n",
    "* To choose the best threshold for signal probability, we shall use a separate validation set.\n",
    "* We use a simple grid search. If you want something bigger, go for [rep.metrics.OptimalAMS](https://yandex.github.io/rep/report.html#rep.report.metrics.OptimalAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ams(s, b, br=10.):\n",
    "    \"\"\"\n",
    "    Regularized approximate median significance\n",
    "\n",
    "    :param s: amount of signal passed\n",
    "    :param b: amount of background passed\n",
    "    :param br: regularization\n",
    "    \"\"\"\n",
    "    radicand = 2 * ((s + b + br) * np.log(1.0 + s / (b + br)) - s)\n",
    "    return np.sqrt(radicand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done   1 out of 242 | elapsed:    0.0s remaining:   11.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 160 | elapsed:    0.0s remaining:    3.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  76 | elapsed:    0.0s remaining:    3.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 126 | elapsed:    0.0s remaining:    4.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  74 | elapsed:    0.0s remaining:    2.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 119 | elapsed:    0.1s remaining:    7.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 168 | elapsed:    0.0s remaining:    6.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  15 | elapsed:    0.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 183 | elapsed:    0.0s remaining:    3.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  12 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 100 | elapsed:    0.0s remaining:    2.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  14 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 144 | elapsed:    0.1s remaining:    7.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 113 | elapsed:    0.0s remaining:    4.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 292 | elapsed:    0.0s remaining:    9.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  13 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  54 | elapsed:    0.0s remaining:    2.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  38 | elapsed:    0.0s remaining:    1.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 183 | elapsed:    0.0s remaining:    5.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  82 | elapsed:    0.0s remaining:    1.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#predict signal probabilities for validation set\n",
    "Yval_proba = classifier.predict_proba(Xval)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Yval_proba_sort = Yval_proba[np.argsort(Yval_proba)]\n",
    "Yval_sort = Yval[np.argsort(Yval_proba)]\n",
    "Wval_sort = Wval[np.argsort(Yval_proba)]\n",
    "\n",
    "min_quantile = 0.0\n",
    "max_quantile = 0.99\n",
    "\n",
    "i_min = int(len(Yval_proba_sort)*min_quantile)\n",
    "i_max = int(len(Yval_proba_sort)*max_quantile)\n",
    "\n",
    "s = (Yval_sort[i_min:]*Wval_sort[i_min:]).sum()\n",
    "b = ((1-Yval_sort[i_min:])*Wval_sort[i_min:]).sum()\n",
    "\n",
    "ams_steps = [ams(s,b)]\n",
    "for i in range(i_min+1,i_max):\n",
    "    if Yval_sort[i]:\n",
    "        s -=Wval_sort[i]\n",
    "    else:\n",
    "        b -=Wval_sort[i]\n",
    "    ams_steps.append(ams(s,b))\n",
    "    \n",
    "quantiles = np.arange(i_min,i_max) / float( len(Yval))\n",
    "optimal_quantile = quantiles[np.argmax(ams_steps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot AMS against cut percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal quantile threshold: 0.84407\n",
      "Optimal AMS: 3.74893698742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEKCAYAAAD3tSVSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ab9920bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(quantiles,ams_steps)\n",
    "plt.title(\"Validation AMS against quantile threshold\")\n",
    "print \"Optimal quantile threshold:\",optimal_quantile\n",
    "print \"Optimal AMS:\",np.max(ams_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weasel(x)\n",
    "optimal_quantile = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, score the test set\n",
    "* Using only the parameters learned on training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   29.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   25.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 107 | elapsed:    0.1s remaining:   11.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 188 | elapsed:    0.1s remaining:   14.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   24.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  35 | elapsed:    0.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   29.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   28.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   16.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   21.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   19.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   34.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   27.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   27.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   37.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 245 | elapsed:    0.1s remaining:   17.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 172 | elapsed:    0.1s remaining:   10.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   32.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 204 | elapsed:    0.1s remaining:   10.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 225 | elapsed:    0.1s remaining:   17.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "Ytest_proba = classifier.predict_proba(Xtest)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability threshold for test: 0.444921440795\n"
     ]
    }
   ],
   "source": [
    "threshold = np.percentile(Ytest_proba,85)\n",
    "print \"Probability threshold for test:\",threshold\n",
    "\n",
    "\n",
    "Ytest_class = Ytest_proba >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s= 224.130284547 b= 3520.7867428\n",
      "AMS = 3.73305366428\n"
     ]
    }
   ],
   "source": [
    "selection_Y = Ytest[Ytest_class==1]\n",
    "selection_weights = Wtest[Ytest_class==1]\n",
    "\n",
    "s = selection_Y.dot(selection_weights)\n",
    "b = (1 - selection_Y).dot(selection_weights)\n",
    "print 's=',s,'b=',b\n",
    "print \"AMS =\", ams(s,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
