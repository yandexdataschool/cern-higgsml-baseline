{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiggsML baseline solution\n",
    "\n",
    "This code walks you through the baseline solution for higgs competition\n",
    "1. Downloading the data\n",
    "2. Preprocessing\n",
    "3. Training model\n",
    "4. Saving predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data\n",
    "Just a simple bash script to load kaggle higgs dataset.\n",
    "\n",
    "First run may take a few minutes to actually download the data from `opendata.cern.ch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [[ ! -f 'atlas-higgs-challenge-2014-v2.csv' ]]; then \\\n",
    "    echo \"downloading dataset...\"\n",
    "    wget -c http://opendata.cern.ch/record/328/files/atlas-higgs-challenge-2014-v2.csv.gz ; \\\n",
    "    gunzip 'atlas-higgs-challenge-2014-v2.csv.gz' ; \\\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read as pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_csv('atlas-higgs-challenge-2014-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100005</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "      <td>0.025434</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>0.083414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100006</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100007</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.018636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100008</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.614803</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.296003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100009</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>28.859</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>167.735</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.001502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "5   100005        89.744                       13.550        59.149   116.344   \n",
       "6   100006       148.754                       28.862       107.782   106.130   \n",
       "7   100007       154.916                       10.418        94.714    29.169   \n",
       "8   100008       105.594                       50.559       100.989     4.288   \n",
       "9   100009       128.053                       88.941        69.272   193.392   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                 0.910           124.711                2.666   \n",
       "1              -999.000          -999.000             -999.000   \n",
       "2              -999.000          -999.000             -999.000   \n",
       "3              -999.000          -999.000             -999.000   \n",
       "4              -999.000          -999.000             -999.000   \n",
       "5                 2.636           284.584               -0.540   \n",
       "6                 0.733           158.359                0.113   \n",
       "7              -999.000          -999.000             -999.000   \n",
       "8              -999.000          -999.000             -999.000   \n",
       "9              -999.000          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "5               1.362      61.619      ...                    -2.412   \n",
       "6               2.941       2.545      ...                     0.864   \n",
       "7               2.897       1.526      ...                    -0.715   \n",
       "8               2.904       4.288      ...                  -999.000   \n",
       "9               1.609      28.859      ...                    -2.767   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                   1.240   \n",
       "1                1.158               -999.000                -999.000   \n",
       "2               -2.028               -999.000                -999.000   \n",
       "3             -999.000               -999.000                -999.000   \n",
       "4             -999.000               -999.000                -999.000   \n",
       "5               -0.653                 56.165                   0.224   \n",
       "6                1.450                 56.867                   0.131   \n",
       "7               -1.724               -999.000                -999.000   \n",
       "8             -999.000               -999.000                -999.000   \n",
       "9               -2.514               -999.000                -999.000   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      s          t   \n",
       "1                -999.000          46.226  0.681042      b          t   \n",
       "2                -999.000          44.251  0.715742      b          t   \n",
       "3                -999.000          -0.000  1.660654      b          t   \n",
       "4                -999.000           0.000  1.904263      b          t   \n",
       "5                   3.106         193.660  0.025434      b          t   \n",
       "6                  -2.767         179.877  0.000814      s          t   \n",
       "7                -999.000          30.638  0.005721      s          t   \n",
       "8                -999.000           0.000  1.614803      b          t   \n",
       "9                -999.000         167.735  0.000461      s          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "5      0.083414  \n",
       "6      0.002653  \n",
       "7      0.018636  \n",
       "8      5.296003  \n",
       "9      0.001502  \n",
       "\n",
       "[10 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a glimpse into the dataset\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "- We apply a couple of standard data analysis techniques\n",
    "  * Selecting the feature __(X)__, target __(y)__ and __weight__ columns \n",
    "  * Imputing missing values\n",
    "  * Adding logarithmic features\n",
    "  * Splitting data into training, test and validation sets\n",
    "\n",
    "\n",
    "You may note that some of the features are redundant for the models we are going to use.\n",
    "Nevertheless, they are computed so that you could jump to your own models quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#target column\n",
    "y = df['Label'].values == 's'\n",
    "\n",
    "#features\n",
    "X_raw = df.iloc[:,1:31].values\n",
    "\n",
    "#weights\n",
    "weights = df['KaggleWeight'].values\n",
    "\n",
    "#whether sample is in training or test set\n",
    "dataset_mask = df[\"KaggleSet\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't like the perspective of waiting that long each time, use \n",
    "* `X_imputed.to_csv(\"myfile.csv\")` to save\n",
    "* `X_imputed = pd.DataFrame.from_csv(\"myfile.csv\")` to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logarithmic features\n",
    "* Pick all strictly positive columns and add $ log(1 + v) $ transformations of these features.\n",
    "* This has only a minor effect for tree methods used here, however it will simplify trying out other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X columns with strictly positive features (selected manually)\n",
    "positive_features = (0,1,2,3,4,5,7,8,9,10,12,13,16,19,21,23,26)\n",
    "\n",
    "X_log = np.log(1 + X_raw[:,positive_features])\n",
    "X_log[np.isnan(X_log)] = -999\n",
    "\n",
    "X = np.hstack((X_raw, X_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/test split\n",
    "* Split data into 3 sets: training, validation and the final test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtr,Ytr,Wtr = map(lambda v: v[dataset_mask=='t'], [X,y,weights])\n",
    "Xval,Yval,Wval = map(lambda v: v[dataset_mask=='b'], [X,y,weights])\n",
    "Xtest,Ytest,Wtest = map(lambda v: v[dataset_mask=='v'], [X,y,weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale to equalize feature variances\n",
    "* This too is hardly applicable to the tree ensembles, but other models may benefit from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr,Xval,Xtest = map(scaler.transform, (Xtr,Xval,Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model\n",
    "\n",
    "Here we define and train a custom ensemble of decision trees.\n",
    "* Namely, we use AdaBoost algorithm above Extremely Randomized Trees ensembles.\n",
    "\n",
    "The idea behind [Extremely Randomized Trees](https://pdfs.semanticscholar.org/336a/165c17c9c56160d332b9f4a2b403fccbdbfb.pdf) is to spawn many weak decision trees using random subsamples of data and features, and than average them out in hope to get a quality above any individual tree.\n",
    "\n",
    "[AdaBoost](http://rob.schapire.net/papers/explaining-adaboost.pdf), in it's turn, trains estimators sequentially, each new one laying more emphasis on the data samples that previous estimators had most problems with. This is done through iterative reweighting of data samples based on the ensemble score on these samples.\n",
    "\n",
    "\n",
    "On each iteration of AdaBoost, we train not one, but several extremely randomized trees using AdaBoost-reweighted samples. We use the extremely randomized trees parallelism to spawn a lot of decision trees in less CPU time and AdaBoost steps to 'guide' the new trees to most important samples. \n",
    "\n",
    "\n",
    "We have taken initial inner model parameters from several Kaggle Higgs Boson Competition forum discussions and made a few steps with sheer gut feeling.\n",
    "We have initialized AdaBoost params with numbers whispered by a weasel ghost, and after a few tweaks they turned out to work ~not too badly.\n",
    "\n",
    "The final model has 300*20=6000 low-quality trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "# The inner Extremely Randomized Trees ensemble to be trained at each AdaBoost step\n",
    "rf = ExtraTreesClassifier(\n",
    "            n_estimators = 300,\n",
    "            max_features = 30,\n",
    "            max_depth = 12,\n",
    "            min_samples_leaf = 100,\n",
    "            min_samples_split = 100,\n",
    "            verbose = 1,\n",
    "            n_jobs = -1 #<- this flag allows to use all CPU cores available. Remove if you don't want that.\n",
    ")\n",
    "\n",
    "# The AdaBoost is defined here\n",
    "classifier = AdaBoostClassifier(\n",
    "        n_estimators = 20,\n",
    "        learning_rate = 0.75,\n",
    "        base_estimator = rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.0s remaining: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   28.8s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.4s remaining: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  74 | elapsed:    0.1s remaining:    4.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.6s remaining: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 198 | elapsed:    0.1s remaining:   10.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.6s remaining: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   41.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.2s remaining: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   20.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.8s remaining: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  88 | elapsed:    0.1s remaining:    5.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.8s remaining: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 263 | elapsed:    0.0s remaining:   12.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.5s remaining: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   27.8s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    5.1s remaining: 25.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  16 | elapsed:    0.0s remaining:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.2s remaining: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 222 | elapsed:    0.1s remaining:   12.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.2s remaining: 20.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   27.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.9s remaining: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  42 | elapsed:    0.0s remaining:    1.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.3s remaining: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 105 | elapsed:    0.1s remaining:    6.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    3.8s remaining: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 117 | elapsed:    0.1s remaining:    6.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.1s remaining: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   24.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.4s remaining: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 285 | elapsed:    0.0s remaining:   14.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.3s remaining: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  32 | elapsed:    0.1s remaining:    1.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.6s remaining: 23.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  43 | elapsed:    0.0s remaining:    1.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.8s remaining: 23.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 175 | elapsed:    0.1s remaining:   11.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of 300 | elapsed:    4.5s remaining: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 113 | elapsed:    0.1s remaining:    7.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fit\n",
      "CPU times: user 6h 35min 33s, sys: 35.5 s, total: 6h 36min 9s\n",
      "Wall time: 25min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"model fit\"\n",
    "classifier.fit(Xtr, Ytr, sample_weight = Wtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate AMS threshold\n",
    "* Approximate Median significance is a threshold-dependent metric\n",
    "* In other words, it requires you to assign \"signal\" vs \"background\" class, not just probabilities.\n",
    "* To choose the best threshold for signal probability, we shall use a separate validation set.\n",
    "* We use a simple grid search. If you want something bigger, go for [rep.metrics.OptimalAMS](https://yandex.github.io/rep/report.html#rep.report.metrics.OptimalAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ams(s, b, br=10.):\n",
    "    \"\"\"\n",
    "    Regularized approximate median significance\n",
    "\n",
    "    :param s: amount of signal passed\n",
    "    :param b: amount of background passed\n",
    "    :param br: regularization\n",
    "    \"\"\"\n",
    "    radicand = 2 * ((s + b + br) * np.log(1.0 + s / (b + br)) - s)\n",
    "    return np.sqrt(radicand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done   1 out of  43 | elapsed:    0.1s remaining:    2.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 283 | elapsed:    0.1s remaining:   23.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 161 | elapsed:    0.0s remaining:    6.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 128 | elapsed:    0.0s remaining:    5.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   16.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  17 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.0s remaining:   10.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  97 | elapsed:    0.0s remaining:    3.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  14 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   15.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 261 | elapsed:    0.1s remaining:   15.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 219 | elapsed:    0.0s remaining:   10.8s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  43 | elapsed:    0.0s remaining:    1.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 274 | elapsed:    0.0s remaining:   10.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  79 | elapsed:    0.0s remaining:    2.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  18 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 226 | elapsed:    0.1s remaining:   12.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  34 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  12 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  42 | elapsed:    0.0s remaining:    1.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#predict signal probabilities for validation set\n",
    "Yval_proba = classifier.predict_proba(Xval)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Yval_proba_sort = Yval_proba[np.argsort(Yval_proba)]\n",
    "Yval_sort = Yval[np.argsort(Yval_proba)]\n",
    "Wval_sort = Wval[np.argsort(Yval_proba)]\n",
    "\n",
    "min_quantile = 0.0\n",
    "max_quantile = 0.99\n",
    "\n",
    "i_min = int(len(Yval_proba_sort)*min_quantile)\n",
    "i_max = int(len(Yval_proba_sort)*max_quantile)\n",
    "\n",
    "s = (Yval_sort[i_min:]*Wval_sort[i_min:]).sum()\n",
    "b = ((1-Yval_sort[i_min:])*Wval_sort[i_min:]).sum()\n",
    "\n",
    "ams_steps = [ams(s,b)]\n",
    "for i in range(i_min+1,i_max):\n",
    "    if Yval_sort[i]:\n",
    "        s -=Wval_sort[i]\n",
    "    else:\n",
    "        b -=Wval_sort[i]\n",
    "    ams_steps.append(ams(s,b))\n",
    "    \n",
    "quantiles = np.arange(i_min,i_max) / float( len(Yval))\n",
    "optimal_quantile = quantiles[np.argmax(ams_steps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot AMS against cut percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal quantile threshold: 0.84259\n",
      "Optimal AMS: 3.78065210374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEKCAYAAAD3tSVSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4c49b8bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(quantiles,ams_steps)\n",
    "plt.title(\"Validation AMS against quantile threshold\")\n",
    "print \"Optimal quantile threshold:\",optimal_quantile\n",
    "print \"Optimal AMS:\",np.max(ams_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weasel(x)\n",
    "optimal_quantile = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, score the test set\n",
    "* Using only the parameters learned on training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   30.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   20.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  12 | elapsed:    0.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   24.7s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   21.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   20.8s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   25.9s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 258 | elapsed:    0.1s remaining:   16.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   30.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 239 | elapsed:    0.1s remaining:   15.2s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  56 | elapsed:    0.1s remaining:    4.6s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   22.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   28.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   32.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   26.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   23.0s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   19.5s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 228 | elapsed:    0.1s remaining:   15.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   29.1s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of 300 | elapsed:    0.1s remaining:   32.3s\n",
      "[Parallel(n_jobs=16)]: Done 300 out of 300 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "Ytest_proba = classifier.predict_proba(Xtest)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability threshold for test: 0.444374490174\n"
     ]
    }
   ],
   "source": [
    "threshold = np.percentile(Ytest_proba,85)\n",
    "print \"Probability threshold for test:\",threshold\n",
    "\n",
    "\n",
    "Ytest_class = Ytest_proba >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s= 225.217927938 b= 3536.08772045\n",
      "AMS = 3.74304782679\n"
     ]
    }
   ],
   "source": [
    "selection_Y = Ytest[Ytest_class==1]\n",
    "selection_weights = Wtest[Ytest_class==1]\n",
    "\n",
    "s = selection_Y.dot(selection_weights)\n",
    "b = (1 - selection_Y).dot(selection_weights)\n",
    "print 's=',s,'b=',b\n",
    "print \"AMS =\", ams(s,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
